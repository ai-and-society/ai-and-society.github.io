<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <generator uri="http://jekyllrb.com" version="3.8.4">Jekyll</generator>
  
  
  <link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" />
  <updated>2018-12-10T19:08:34+00:00</updated>
  <id>http://localhost:4000//</id>

  
    <title type="html">MIT AI Ethics</title>
  

  
    <subtitle>Discuss AI and ethics twice a month with the MIT community. Notes and readings posted online.</subtitle>
  

  
    <author>
        <name>Irene Chen</name>
      
      
    </author>
  

  
  
    <entry>
      
      <title type="html">Building AI ethics education into computer science classes</title>
      
      
      <link href="http://localhost:4000/2018/11/07/ethics-education/" rel="alternate" type="text/html" title="Building AI ethics education into computer science classes" />
      
      <published>2018-11-07T00:00:00+00:00</published>
      <updated>2018-11-07T00:00:00+00:00</updated>
      <id>http://localhost:4000/2018/11/07/ethics-education</id>
      <content type="html" xml:base="http://localhost:4000/2018/11/07/ethics-education/">&lt;p&gt;This week in the MIT AI and Ethics group, we discussed the role of ethics in AI and computer science education.  In particular, we discussed two papers: &lt;a href=&quot;https://arxiv.org/pdf/1808.05686.pdf&quot;&gt;an essay on integrating ethics in existing courses&lt;/a&gt; from Harvard, and &lt;a href=&quot;https://www.scu.edu/media/ethics-center/technology-ethics/IntroToDataEthics.pdf&quot;&gt;a curriculum and exercises&lt;/a&gt; from SCU.  Our goal at the end of the session was to provide a set of recommendations to the department, school of computer science, or even the greater institute on how to integrate ethics into education.&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Students debate how to incorporate AI ethics education.&lt;/caption&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/meeting2.jpg&quot; alt=&quot;Students debate how to incorporate AI ethics education.&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;In the beginning session, we asked what it means to be responsible in the context of full-system engineering.  For example, earlier this year, Uber reported the first pedestrian fatally due to a self-driving car.  Despite some faults (no system is truly error proof), some group of people (likely management or manufacturer) decided  the car was suitable to put on the road. In this example, who is responsible, and what does it mean to be responsible?  During the  exercise, some people brought up the analogy between self-driving car accidents and mishaps during surgery – if someone dies after a surgery it doesn’t mean that the inventor of that surgery is at fault.  Other people mentioned that it’s difficult to put fault on a developer since the code in question is often the product of a larger system of teams and managers. Similarly, it is hard to pinpoint a single entity who should have foreseen the error.  This lead to a discussion of how we can teach computer scientists  how to foresee and combat critical edge cases.&lt;/p&gt;

&lt;p&gt;We had a diverse set of people and discussions that emerged.  In one group, someone mentioned a program at Stanford University where you can pair a CS degree with a minor in a humanities  field.  They brought up that perhaps MIT should require that students choose their general education courses to be more cohesive and directed (e.g.,  CS + ethics or CS+ policy).&lt;/p&gt;

&lt;p&gt;This led to a debate about whether ethics should be integrated directly into CS courses, or taught separately.  Many people disagreed with the  integrated approach, noting that it is hard to teach ethics in a CS course.  They brought up that ome professors of CS might not want to teach ethics and may brush it off, but having separate classes would allow ethics experts to teach them.   However, many stated that having separate ethics courses may not be popular with students or teachers.  From prior experience at MIT, one student mentioned that having  separate required classes (like communication-intensive requirements at MIT) may cause students to end up being annoyed or not see the value in a course  they must  take to graduate.&lt;/p&gt;

&lt;p&gt;A group of attendees from the Philosophy department brought up a middle-ground solution that they are currently implementing, that involves an ethics curriculum integrated into existing courses but taught by ethics experts. Their focus in on integrating ethics into fundamental design processes, such that it is a part of an engineering mindset from the outset.  The curriculum focuses on value-sensitive design, examining stakeholders and affected population to form theories and make ethical design choices.  This was contrasted this with a theory-based approach that starts off with a presupposed theory of ethics and then tries to implement it. Some of the participants will be testing out this course over IAP!&lt;/p&gt;

&lt;p&gt;While we focused a lot on college-level education, it was also higlightlighted that ethics should be taught early and often in the curriculum.  As students are first  learning how to code, they should learn a design process (e.g. like writing tests first) that incorporates ethics as part of the workflow.&lt;/p&gt;

&lt;p&gt;Another question that was raised was how to make ethics an ongoing educational discipline.  It’s important  that students realize that ethics is a constant practice and not just a skill that is learned and applied, e.g. “I got an A so I’m good at ethics.”  This was further support for an integrated approach where ethics appears consistently throughout many courses, from problem sets to chapters in textbooks or course notes. We briefly discussed the natural extension of how to  incorporate ethics education into professional workplaces.   If  ethics education is purely tackled in the undergraduate curriculum, then we may be neglecting a cohort of people currently in the workforce who could benefit more immediately.  We discussed how a pathway for this may be an online course that leads to a professional certification.&lt;/p&gt;

&lt;p&gt;Our next session on anonymity and privacy in the context of AI and ethics on Wednesday November 28th.&lt;/p&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Leilani Gilpin</name>
          
          
        </author>
      

      

      

      
        <summary type="html">This week in the MIT AI and Ethics group, we discussed the role of ethics in AI and computer science education. In particular, we discussed two papers: an essay on integrating ethics in existing courses from Harvard, and a curriculum and exercises from SCU. Our goal at the end of the session was to provide a set of recommendations to the department, school of computer science, or even the greater institute on how to integrate ethics into education. Students debate how to incorporate AI ethics education. In the beginning session, we asked what it means to be responsible in the context of full-system engineering. For example, earlier this year, Uber reported the first pedestrian fatally due to a self-driving car. Despite some faults (no system is truly error proof), some group of people (likely management or manufacturer) decided the car was suitable to put on the road. In this example, who is responsible, and what does it mean to be responsible? During the exercise, some people brought up the analogy between self-driving car accidents and mishaps during surgery – if someone dies after a surgery it doesn’t mean that the inventor of that surgery is at fault. Other people mentioned that it’s difficult to put fault on a developer since the code in question is often the product of a larger system of teams and managers. Similarly, it is hard to pinpoint a single entity who should have foreseen the error. This lead to a discussion of how we can teach computer scientists how to foresee and combat critical edge cases. We had a diverse set of people and discussions that emerged. In one group, someone mentioned a program at Stanford University where you can pair a CS degree with a minor in a humanities field. They brought up that perhaps MIT should require that students choose their general education courses to be more cohesive and directed (e.g., CS + ethics or CS+ policy). This led to a debate about whether ethics should be integrated directly into CS courses, or taught separately. Many people disagreed with the integrated approach, noting that it is hard to teach ethics in a CS course. They brought up that ome professors of CS might not want to teach ethics and may brush it off, but having separate classes would allow ethics experts to teach them. However, many stated that having separate ethics courses may not be popular with students or teachers. From prior experience at MIT, one student mentioned that having separate required classes (like communication-intensive requirements at MIT) may cause students to end up being annoyed or not see the value in a course they must take to graduate. A group of attendees from the Philosophy department brought up a middle-ground solution that they are currently implementing, that involves an ethics curriculum integrated into existing courses but taught by ethics experts. Their focus in on integrating ethics into fundamental design processes, such that it is a part of an engineering mindset from the outset. The curriculum focuses on value-sensitive design, examining stakeholders and affected population to form theories and make ethical design choices. This was contrasted this with a theory-based approach that starts off with a presupposed theory of ethics and then tries to implement it. Some of the participants will be testing out this course over IAP! While we focused a lot on college-level education, it was also higlightlighted that ethics should be taught early and often in the curriculum. As students are first learning how to code, they should learn a design process (e.g. like writing tests first) that incorporates ethics as part of the workflow. Another question that was raised was how to make ethics an ongoing educational discipline. It’s important that students realize that ethics is a constant practice and not just a skill that is learned and applied, e.g. “I got an A so I’m good at ethics.” This was further support for an integrated approach where ethics appears consistently throughout many courses, from problem sets to chapters in textbooks or course notes. We briefly discussed the natural extension of how to incorporate ethics education into professional workplaces. If ethics education is purely tackled in the undergraduate curriculum, then we may be neglecting a cohort of people currently in the workforce who could benefit more immediately. We discussed how a pathway for this may be an online course that leads to a professional certification. Our next session on anonymity and privacy in the context of AI and ethics on Wednesday November 28th.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Using AI to predict sexual orientation from faces</title>
      
      
      <link href="http://localhost:4000/2018/10/24/predict-sexuality/" rel="alternate" type="text/html" title="Using AI to predict sexual orientation from faces" />
      
      <published>2018-10-24T00:00:00+00:00</published>
      <updated>2018-10-24T00:00:00+00:00</updated>
      <id>http://localhost:4000/2018/10/24/predict-sexuality</id>
      <content type="html" xml:base="http://localhost:4000/2018/10/24/predict-sexuality/">&lt;p&gt;It is undeniable that artificial intelligence (AI) has become increasingly more powerful and influential in our society. On the heels of &lt;a href=&quot;http://news.mit.edu/2018/mit-reshapes-itself-stephen-schwarzman-college-of-computing-1015&quot;&gt;MIT’s recent investment in a new college for computing&lt;/a&gt;, we created the AI Ethics Reading Group to build a university-wide community to discuss foundation and recent literature at the intersection of AI and society.&lt;/p&gt;

&lt;p&gt;Our group will meet every other week and discuss readings on a different topic. Participants from all over the MIT community are welcome, and we hope that a wide range of people will lead discussions based on different areas of interest and expertise. For more information on the meeting times, see &lt;a href=&quot;https://mitaiethics.github.io/&quot;&gt;our main site&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With over 150 participants on the newly formed email list, we held our first meeting on Wed Oct 24. While nothing can replicate the energetic interactions of an actual reading group meeting, we hope that we are able to capture some of the discussion points here. Also check out &lt;a href=&quot;https://docs.google.com/presentation/d/1kQ02D0c2SNCOWj3nErSUnSrjkcjB2aWBE2Oh4f7WPZU/edit?usp=sharing&quot;&gt;our slides from the meeting&lt;/a&gt;.&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
&lt;caption align=&quot;bottom&quot;&gt;Eager minds in active discussion about the ethics of predicting sexual orientation from faces.&lt;/caption&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/meeting1.jpg&quot; alt=&quot;Eager minds in active discussion about the ethics of predicting sexual orientation from faces.&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;We had a healthy turnout of over 60 people on our first meeting – not bad for sending out the first email a few days beforehand. After introductions and logistics for the reading group, we opened the floor with a few thought experiments. People were asked to align themselves across the room according to how they felt about each question and later briefly explain their thinking.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Cash bail currently leaves innocent people in jail waiting for a trial because they’re too poor to pay bail. &lt;a href=&quot;https://www.economist.com/united-states/2017/11/23/replacing-bail-with-an-algorithm&quot;&gt;California recently passed legislation around risk assessment tools&lt;/a&gt; that assign scores based on age, employment, drug use, and criminal history; however similar algorithms have been shown to produced biased recommendations. &lt;strong&gt;Should states adopt legislation to reduce the cash bail system in favor of the risk assessment scores?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Companies are often overwhelmed by job applications, relying on humans (who may have their own biases) to read through and filter. &lt;a href=&quot;https://qz.com/1427621/companies-are-on-the-hook-if-their-hiring-algorithms-are-biased/&quot;&gt;Recent attempts at a resume algorithm to screen applicants&lt;/a&gt; (e.g. at Amazon) were shown to be biased against terms involving women like “women’s chess champion” and biased for men named Jared who played high school lacrosse. &lt;strong&gt;If you were CEO of a company, would you use algorithmic resume screening or stick with human resume screening?&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Readings and Discussion&lt;/h2&gt;
&lt;p&gt;Our first set of readings covered the prediction task of predicting sexual orientation from faces.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual&quot;&gt;Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477&quot;&gt;Do algorithms reveal sexual orientation or just expose our stereotypes?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first reading described a deep learning experiment &lt;em&gt;predicting sexual orientation from dating app photos&lt;/em&gt;. The authors Kosinski and Wang then used features extracted from a deep learning model and trained a logistic regression to achieve an area under the receiving operator curve (AUC) of 0.81 for men compared to an AUC of 0.71 for women. Using heat maps to uncover areas of highest signal, the authors then connected the results to &lt;em&gt;prenatal hormone theory (PHT)&lt;/em&gt; which suggests that exposure to certain prenatal hormones can shape a person’s sexual orientation and physiognomy. The model then identified so-called &lt;em&gt;fixed characteristics&lt;/em&gt; (e.g. nose shape and jaw size) and &lt;em&gt;transient facial characteristics&lt;/em&gt; (e.g. makeup, skin tone) most predictive of sexual orientation.&lt;/p&gt;

&lt;p&gt;In response to the findings presented in Kosinski and Wang (2018), the second reading delved into the experiment and highlighted other explanations for the findings. Using Amazon’s Mechanical Turk crowdsourcing platform, the researchers Arcas, Todorov, and Mitchell found that &lt;em&gt;many of the original findings may be related to preferences instead of physiognomy&lt;/em&gt;. For example, while gay men were more likely to wear glasses, the differences are not due to visual acuity but instead due to liking how they looked in glasses more. Similar results were found for facial hair preferences and enjoyment for the outdoors which causes darker skin. Lastly, the authors outlined a host of &lt;em&gt;factors that may change the predicted sexual orientation for the same face&lt;/em&gt; including makeup, eyeshadow, facial hair, glasses, selfie angle, and sun exposure.&lt;/p&gt;

&lt;p&gt;With a large crowd of participants and many questions to debate, we split into smaller groups of 3-8 people with suggested discussion topics. After almost an hour of dialogue in the smaller groups, we reconvened in a larger group to share thoughts. Below are some of the opinions and concepts raised.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;It’s hard to find a single positive use case for the first paper. The model is also limited by the use of dating profile pictures as training data, which makes generalization to other settings like Facebook photos challenging.&lt;/li&gt;
  &lt;li&gt;One idea of how the authors of the first paper could have presented the ideas better is through separating ideas into a machine learning paper and a science paper in order to fully explore the implications and potential explanations for the findings.&lt;/li&gt;
  &lt;li&gt;At best the algorithm to predict sexual orientation is finding spurious correlations in the data, which one participant described as evil.&lt;/li&gt;
  &lt;li&gt;Some questions the authors bring up have typically been studied from a broader, humanistic perspective; that broader perspective seems to be neglected here.  Some problems (perhaps especially those relating to the intimate experiences of people) may be ill-suited to a rigid scientific/computer science methodology.&lt;/li&gt;
  &lt;li&gt;More generally, the incentive structure in computer science academia involves lots of machine learning conferences each year, expectations to submit and publish at multiple of these, and standard ways to prove success (e.g., increasing accuracy on a benchmark).  This may discourage rigorous analysis, transparency, and thoughtful consideration of ethics.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Looking forward&lt;/h2&gt;

&lt;p&gt;In our first iteration of the reading group, we learned a few lessons to keep in mind for following meetings.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Keep discussions specific.&lt;/strong&gt; Discussing the pros and cons of cash bail and risk assessment scores is more interesting when discussing one state’s pros and cons when considering which law to pass. Debating whether or not to use biased algorithms feels more nebulous and can end with grand statements with fewer opportunities for others to engage.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Establish norms for debate.&lt;/strong&gt; Inevitably discussions can get heated, especially while discussing charged topics. It’s important to set ground rules early to maintain an inclusive, respectful, and considerate environment.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Beginners and experts welcome.&lt;/strong&gt; At first, I was apprehensive that the three organizers were computer science PhD students without any ethics backgrounds. Luckily, the MIT community has a large crowd of perspectives to lend, and both beginners and experts had plenty to share.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We also polled informally for future topics to wrestle together.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Autonomous weapons&lt;/strong&gt;: As AI and warfare progress together, soon we will be able to deploy lethal autonomous weapons systems. Should these be banned and on what basis? If banned, how do we prevent underground research from creating them anyway?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Privacy&lt;/strong&gt;: With ubiquitous AI systems, where is the line for individual privacy? How can someone opt-out of AI? What level of surveillance should be acceptable?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ranking of forum posts&lt;/strong&gt;: Currently in online discussion platforms, majority rule for forum post rankings creates a winner-take-all system. How can we showcase a diversity of perspectives in ranking algorithms?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Synthetic data&lt;/strong&gt;: The ability to create fake images, videos, and news stories could have wide reaching implications for the future of truth. How can we detect, regulate, and protect against this troubling line of research?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AI human rights&lt;/strong&gt;: When algorithms become incredibly advanced so much so that they are almost human-like, how should we think about humane treatment? What is suffering for an algorithm?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AI and jobs&lt;/strong&gt;: As more jobs become replaced by algorithms – particularly repetitive or other easily automated professions – how can we create room for people to retrain and assume new careers?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inequality and AI&lt;/strong&gt;: Access to AI has widened an existing wealth gap as companies can now reap benefits without employing nearly as many workers. Relatedly, the computing resources to be able to run deep learning models can be a barrier to entry for some communities, countries, and continents. How should we distribute wealth created by machines?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Overall, we were thrilled with the turnout for the first meeting, and hope everyone joins us for the next meeting on Wednesday November 7.&lt;/p&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Irene Chen</name>
          
          
        </author>
      

      

      

      
        <summary type="html">It is undeniable that artificial intelligence (AI) has become increasingly more powerful and influential in our society. On the heels of MIT’s recent investment in a new college for computing, we created the AI Ethics Reading Group to build a university-wide community to discuss foundation and recent literature at the intersection of AI and society. Our group will meet every other week and discuss readings on a different topic. Participants from all over the MIT community are welcome, and we hope that a wide range of people will lead discussions based on different areas of interest and expertise. For more information on the meeting times, see our main site. With over 150 participants on the newly formed email list, we held our first meeting on Wed Oct 24. While nothing can replicate the energetic interactions of an actual reading group meeting, we hope that we are able to capture some of the discussion points here. Also check out our slides from the meeting. Eager minds in active discussion about the ethics of predicting sexual orientation from faces. Preliminaries We had a healthy turnout of over 60 people on our first meeting – not bad for sending out the first email a few days beforehand. After introductions and logistics for the reading group, we opened the floor with a few thought experiments. People were asked to align themselves across the room according to how they felt about each question and later briefly explain their thinking. Cash bail currently leaves innocent people in jail waiting for a trial because they’re too poor to pay bail. California recently passed legislation around risk assessment tools that assign scores based on age, employment, drug use, and criminal history; however similar algorithms have been shown to produced biased recommendations. Should states adopt legislation to reduce the cash bail system in favor of the risk assessment scores? Companies are often overwhelmed by job applications, relying on humans (who may have their own biases) to read through and filter. Recent attempts at a resume algorithm to screen applicants (e.g. at Amazon) were shown to be biased against terms involving women like “women’s chess champion” and biased for men named Jared who played high school lacrosse. If you were CEO of a company, would you use algorithmic resume screening or stick with human resume screening? Readings and Discussion Our first set of readings covered the prediction task of predicting sexual orientation from faces. Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images Do algorithms reveal sexual orientation or just expose our stereotypes? The first reading described a deep learning experiment predicting sexual orientation from dating app photos. The authors Kosinski and Wang then used features extracted from a deep learning model and trained a logistic regression to achieve an area under the receiving operator curve (AUC) of 0.81 for men compared to an AUC of 0.71 for women. Using heat maps to uncover areas of highest signal, the authors then connected the results to prenatal hormone theory (PHT) which suggests that exposure to certain prenatal hormones can shape a person’s sexual orientation and physiognomy. The model then identified so-called fixed characteristics (e.g. nose shape and jaw size) and transient facial characteristics (e.g. makeup, skin tone) most predictive of sexual orientation. In response to the findings presented in Kosinski and Wang (2018), the second reading delved into the experiment and highlighted other explanations for the findings. Using Amazon’s Mechanical Turk crowdsourcing platform, the researchers Arcas, Todorov, and Mitchell found that many of the original findings may be related to preferences instead of physiognomy. For example, while gay men were more likely to wear glasses, the differences are not due to visual acuity but instead due to liking how they looked in glasses more. Similar results were found for facial hair preferences and enjoyment for the outdoors which causes darker skin. Lastly, the authors outlined a host of factors that may change the predicted sexual orientation for the same face including makeup, eyeshadow, facial hair, glasses, selfie angle, and sun exposure. With a large crowd of participants and many questions to debate, we split into smaller groups of 3-8 people with suggested discussion topics. After almost an hour of dialogue in the smaller groups, we reconvened in a larger group to share thoughts. Below are some of the opinions and concepts raised. It’s hard to find a single positive use case for the first paper. The model is also limited by the use of dating profile pictures as training data, which makes generalization to other settings like Facebook photos challenging. One idea of how the authors of the first paper could have presented the ideas better is through separating ideas into a machine learning paper and a science paper in order to fully explore the implications and potential explanations for the findings. At best the algorithm to predict sexual orientation is finding spurious correlations in the data, which one participant described as evil. Some questions the authors bring up have typically been studied from a broader, humanistic perspective; that broader perspective seems to be neglected here. Some problems (perhaps especially those relating to the intimate experiences of people) may be ill-suited to a rigid scientific/computer science methodology. More generally, the incentive structure in computer science academia involves lots of machine learning conferences each year, expectations to submit and publish at multiple of these, and standard ways to prove success (e.g., increasing accuracy on a benchmark). This may discourage rigorous analysis, transparency, and thoughtful consideration of ethics. Looking forward In our first iteration of the reading group, we learned a few lessons to keep in mind for following meetings. Keep discussions specific. Discussing the pros and cons of cash bail and risk assessment scores is more interesting when discussing one state’s pros and cons when considering which law to pass. Debating whether or not to use biased algorithms feels more nebulous and can end with grand statements with fewer opportunities for others to engage. Establish norms for debate. Inevitably discussions can get heated, especially while discussing charged topics. It’s important to set ground rules early to maintain an inclusive, respectful, and considerate environment. Beginners and experts welcome. At first, I was apprehensive that the three organizers were computer science PhD students without any ethics backgrounds. Luckily, the MIT community has a large crowd of perspectives to lend, and both beginners and experts had plenty to share. We also polled informally for future topics to wrestle together. Autonomous weapons: As AI and warfare progress together, soon we will be able to deploy lethal autonomous weapons systems. Should these be banned and on what basis? If banned, how do we prevent underground research from creating them anyway? Privacy: With ubiquitous AI systems, where is the line for individual privacy? How can someone opt-out of AI? What level of surveillance should be acceptable? Ranking of forum posts: Currently in online discussion platforms, majority rule for forum post rankings creates a winner-take-all system. How can we showcase a diversity of perspectives in ranking algorithms? Synthetic data: The ability to create fake images, videos, and news stories could have wide reaching implications for the future of truth. How can we detect, regulate, and protect against this troubling line of research? AI human rights: When algorithms become incredibly advanced so much so that they are almost human-like, how should we think about humane treatment? What is suffering for an algorithm? AI and jobs: As more jobs become replaced by algorithms – particularly repetitive or other easily automated professions – how can we create room for people to retrain and assume new careers? Inequality and AI: Access to AI has widened an existing wealth gap as companies can now reap benefits without employing nearly as many workers. Relatedly, the computing resources to be able to run deep learning models can be a barrier to entry for some communities, countries, and continents. How should we distribute wealth created by machines? Overall, we were thrilled with the turnout for the first meeting, and hope everyone joins us for the next meeting on Wednesday November 7.</summary>
      

      
      
    </entry>
  
  
</feed>
